# Prove-MLE-Challenge
 
This project was built for the Prove MLE coding challenge. The program is ran on a CLI when used to predict a user from keystroke input data. The CLI uses a command line input to work and was designed to download a test set from a URL, like the sample test set provided. The algorithm for unpacking the data is also designed with the assumption that the input test set will keep the same format as the sample test given. The model is a neural network that is trained from downloaded JSON data of valid multiple users. It also outputs a list of users that qualified to be used as training data and a list of users who aren't (valid and invalid users). Once downloaded, the data is manipulated into a NumPy array. The labels are one-hot encoded. Then, the network is trained. The accuracy of this specific model is ~57% when used on testing data derived from the training set. The general idea of this model was inspired by a previous Multiclass Classification project.

## Performance Report

The model is a neural network that takes as input an array of keystroke input time. It then outputs a prediction of the user that made the input keystrokes from 35 valid users. The performance of the model is ~57% acccuracy of a testing set derived from the training set. However, it is always able to detect invalid inputs. Seen below is a graph of distribution of the model's prediction over the given test set.

![Distribution Graph](Figure_1.png)

As can be seen, the model predicts that most input from the test set is an invalid input, where the combination of the keystrokes and backspaces don't equate to the sentence "**Be Authentic. Be Yourself. Be Typing.**". From the training set, users are qualified as valid data for training the model if the have 300 or more valid recorded string inputs. with these criteria, it can be seen in the graph below that there are 35 valid users and 5 invalid users. That means that 87.5% of the training set is valid.

![Valid and Invalid users](Figure_2.png)

## Additional Questions

### If you had one additional day, what would you change or improve to your submission?<br />
    If I had one additional day, I would want to improve my data unpacking method. Having never worked with this format of data, it took me a long timee to figure out how I wanted to approach loading the data sets for training the model. Having already done a previous project on Multiclass Classification, I had a rough idea of what I wanted my model to look like. However, getting the data from a raw JSON format to a format that I liked posed a huge challenge and I would've liked to have more time to brainstorm more ways to achieve this.<br />
### How would you modify your solution if the number of users was 1,000 times larger?<br />
    If the number of users was 1,000 times larger, I would want to somehow try and vectorize the data manipulation. When dealing with large sets of data, it becomes very inefficient to traverse the entire set and apply the needed transformations. I would want to vectorize the data in order to apply any transormation as a whole, on the entire data set. Another way I would modify my solution would be to apply time series forecasting. Since this data is time stamped, it is possible to apply time series analysis on it. The reason why I didn't do this was the fact that I had thought of the solution too late into the challenge to pivot and I had also already done some Multiclass Classification, in the past, which led me to try and apply that solution to this problem.<br />
### What insights and takeaways do you have on the distribution of user performance?<br />
    The insights and takeaways on the distribution of user performance, with the current iteration of the model, is that a lot of the test inputs are invalid, meaning the entire combination of keystrokes and backspaces doesn't equate to the sentence "**Be Authentic. Be Yourself. Be Typing.**". Other insight is also that there doesn't seem to be any input from the users 4a45245cde4e11e9b51bacde48001122 or 4a4bd05ede4e11e987bfacde48001122.<br />
### What aspect(s) would you change about this challenge?<br />
    The aspects of this challenge I would change is to provide some way for participants to check the validity of their model onn the sample set. What I mean by this is, currently, we input the sample test to the model and the model predicts a certain user or that the input is invalid. While a participant can see whether or not an input is indeed invalid, they cannot gain any insight on whether or not the model predicted the right user. Another aspect that I would change is having someone to communicate to. While it is very understandable that no full-time engineer should dedicated time in their day to help out an employee candidate, I believe that it emulates a workplace environment a lot more where an intern can seek guidance from a more experienced member.<br />
### What aspect(s) did you enjoy about this challenge?<br />
    The aspects that I enjoyed about this challenge was the exposure I got to a brand new data format. In addition to this new format, the challenge was given in an environment that emulated the work environment in a real job. This meant that I had the chance to search for solutions to problems I haven't seen before, something that is common in a workplace, as opposed to a timed coding challenge. Another aspect that I liked was the experience of meeting a problem that I haven't seen before, outside of a school setting. This allowed me to really dig into my knowledge-base and try to find a solution that worked for a similar problem and adjust it to the current problem. Finally, I really found the premise of the problem very interesting: using time stamped keystrokes to classify inputs into users. It definitely made me more interested in interning at Prove.